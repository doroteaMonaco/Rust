# Esame 2022-10-26

## Domanda 1

Si illustrino le differenze tra stack e heap. Insieme alle differenze, indicare per i seguenti costrutti Rust, in modo dettagliato, dove si trovano i dati che li compongono:

- `Box<[T]>`
- `RefCell<T>`
- `&[T]`

<details>
<summary>Soluzione</summary>

> Soluzione non verificata

La differenza tra stack e heap è la modalità con le quali questi vengono utilizzati e la metodologia secondo cui i dati vengono eliminati:

- lo stack utilizza una struttura dati a pila di tipo LIFO. Le varabili presenti nello stack vengono liberate, se non richiesto esplicitamente prima, ogni qual volta queste escano dal proprio scope (quindi la graffa del blocco che le definisce).
- L'heap ha una struttura di memorizzazione ad albero, dove i dati non vengono liberati all'uscita dallo scope ma su richiesta esplicita da parte del programmatore, motivo per cui è molto importante la gestione della memoria in linguaggi come C/C++ e Rust. Questo ci consente di restituire valori dalle funzioni che sennò verrebbero automaticamente eliminati dal compilatore.

Per quanto riguarda gli smart pointer:

- `Box<[T]>`: Nello stack è presente un puntatore al primo elemento del vettore e un attributo len che indica il numero di valori, nell'heap è presente il vettore.
- `RefCell<T>`: nello stack è presente un campo borrow contenente il campo borrow e il dato T. 
- `&[T]`: riferimento a una slice che può trovarsi sia nello stack che nell'heap. Nello stack sarà presente un puntatore al dato (heap o stack) e un valore len che indica la quantità di dati presenti.

</details>

## Domanda 2

Un sistema concorrente può essere implementato creando più thread nello stesso processo, creando più processi basati su un singolo thread o basati su più thread. Si mettano a confronto i corrispettivi modelli di esecuzione e si evidenzino pregi e difetti in termini di robustezza, prestazioni, scalabilità e semplicità di sviluppo di ciascuno di essi.

<details>
<summary>Soluzione</summary>

> 2.5/3.0 in quanto mancava robustezza e scalabilità

La concorrenza di fatto prevede due possibilità: il meccanismo di concorrenza in single core, dove fondamentalmente ci si contende la risorsa core con lo scheduler che interrompe e fa partire thread, poi c'è la concorrenza in multicore che permette un parallelismo più spinto ossia un parallelismo vero e proprio. 

Dunque la concorrenza nasce con l'idea di parallelizzare i compiti. Un processo con più thread plausibilmente permetterà di parallelizzare i compiti (se si dispone di cpu multicore e la sincronizzazione è correttamente gestita es. non ci sono thread globabi che serializzino i compiti) e dunque svolgerli in minor tempo rispetto ad un approccio mono-thread; va detto che agli inizi non esisteva neanche il concetto di thread (ogni processo era monothread) e si utilizzavano diversi processi e si facevano comunicare tra di loro, il problema però è che i thread condividono lo spazio di indirizzamento (nel contesto di un processo) mentre i processi non condividendo lo spazio di indirizzamento e avendo un isolamento parziale (spazio di indirizzamento non condiviso, ma condivisi file system, periferiche e porte di rete a meno che il processo non sia su un container tipo docker in cui si condivide il kernel ma non tutto il resto). 

Dunque il problema di questo approccio multiprocesso ma monothread crea una difficoltà di comunicazione più costosa perchè ci si deve preoccupare di tradurre i dati con il marshalling. 

Va detto che il problema della concorrenza (immaginiamo un contesto multithread), pone delle problematiche come quelle di ordinamento, visibilità e atomicità, che sono dovute fondamentalmente al modello gerarchico delle cache dei processori multicore. Dunque occorrono delle istruzioni a livello hardware test-and-set, compare-and-swap e barriere di memoria che poi in linguaggi più ad altro livello vengono utilizzate tramite costrutti detti primitive di sincronizzazione (Atomic, Mutex e Condition Variable) e anche qui se non si fa attenzione a utilizzarle adeguatamente si possono creare starvation o addirittura deadlock. In questo Rust ci aiuta poichè ad esempio mette in stretta correlazione la risorsa mutex al valore da proteggere permettendo di fatto al compilatore di segnalarci eventuali problemi (esempio se non stiamo proteggendo adeguatamente una variabile) già in fase di compilazione (fearless concurrency). In un contesto in cui si utilizzano più processi che comunicano ed ogni processo è multi-thread si utilizzera un approccio di scambio di messaggi o condivisione dello stato all'interno dello stesso processo mentre ci si dovrà limitare allo scambio di messaggi tra processi, per il fatto dell'isolamento (ad esempio si potrebbero usare channel tra diversi thread dello stesso processo e pipe o code di messaggi tra diversi processi): realizzazione più complessa in termini di sviluppo.

In termini di robustezza l'approccio mediante processi risulta migliore in quanto impelementato con isolamento, ma per la scalabilità risulta preferibile il multi thread in quanto uno stesso processo può essere scalato in sistemi large scale su più thread.

</details>

## Domanda 3

In riferimento a programmi multi-thread, si indichi se la natura della loro esecuzione sia deterministica o meno a priori. Si produca un esempio che dimostri tale affermazione.

<details>
<summary>Soluzione</summary>

La programmazione multithread (senza sincronizzazione opportuna) non è deterministica si possono ottenere comportamenti imprevedibili ed è la ragione per cui è necessario utilizzare adeguatamente costrutti di sincronizzazione in modo da evitare situazioni di deadlock.

Un esempio che dimostra l'imprevedibilità di un approccio multi-thread è quello dell'**interferenza**, possiamo immaginare una istruzione apparentemente innocua come un increment: `a++`. Nonostante sembri una istruzione sola (atomica) di fatto questa si traduce in due istruzioni: `temp=a; a=temp+1;`.  Ciò porterà poi alla conclusione che non si possono lasciare operazioni in stati intermedi e che non si può fare accesso in lettura/scrittura da più thread contemporaneamente (race condition). Inoltre, a causa del modello gerarchico delle memorie, essendoci cache a diversi livelli che non comunicano tra i core, ci saranno dei tempi di propagazione e quindi di visibilità differenti tra i vari thread di un valore in memoria. Quindi quello che può succedere è che un thread salvi il valore di a in `temp` in una sua locazione di memoria e, e fintanto che questo cambiamento non si propaghi in un altro thread che vuole operare su quel valore, quello che può succedere è che lo scheduler effettui il **freeze** questo primo thread e lo lasci dormiente e nel mentre sveglia un altro thread che fa la stessa operazione salva in un indirizzo di memoria a e poi fa gli increment, questo magari lo fa un pò di volte se sono chiamati più increment. Ma quando viene svegliato il primo thread di fatto riprende la sua esecuzione e mette in `a` il valore del temp che lui aveva incrementato di `1`; quindi stampando il valore sembra che sia tornato indietro. Dunque l'interferenza è uno degli esempi di non determinismo che si possono avere senza i costrutti di sincronizzazione. Un esempio in Rust (che non permette a++) è quello di Rc che avendo i contatori non atomici dei riferimenti (Strong e Weak) non è ne Send ne Sync, infatti per questo è stato creato Arc con i contatori atomici che permettono increment e decrement atomici (operazioni più costose ma necessarie nel multithread).

</details>

## Domanda 4

Un componente con funzionalità di cache permette di ottimizzare il comportamento di un sistema riducendo il numero di volte in cui una funzione è invocata, tenendo traccia dei risultati da essa restituiti a fronte di un particolare dato in ingresso. Per generalità, si assuma che la funzione accetti un dato di tipo generico `K` e restituisca un valore di tipo generico `V`.

Il componente offre un unico metodo `get(...)` che prende in ingresso due parametri, il valore `k` (di tipo K, clonabile) del parametro e la funzione f (di tipo K -> V) responsabile della sua trasformazione, e restituisce uno smart pointer clonabile al relativo valore.

Se, per una determinata chiave `k`, non è ancora stato calcolato il valore corrispondente, la funzione viene invocata e ne viene restituito il risultato; altrimenti viene restituito il risultato già trovato.

Il componente cache deve essere thread-safe perché due o più thread possono richiedere contemporaneamente il valore di una data chiave: quando questo avviene e il dato non è ancora presente, la chiamata alla funzione dovrà essere eseguita nel contesto di UN SOLO thread, mentre gli altri dovranno aspettare il risultato in corso di elaborazione, SENZA CONSUMARE cicli macchina.

Si implementi tale componente a scelta nei linguaggi C++ o Rust.

<!-- 
<details>
<summary>Soluzione</summary>

</details>
-->